{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " K_Fold_HCT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data .."
      ],
      "metadata": {
        "id": "_iR92N4nqvkE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78guAdeqM1j5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmS27XulM9Uv"
      },
      "source": [
        "dataset='/content/drive/MyDrive/Segmented_Augmented'\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHsQtbi7NPNG"
      },
      "source": [
        "cd '/content/drive/MyDrive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qalYKvouNTCc"
      },
      "source": [
        "pip install -U tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYIcxw09NTjn"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "##\n",
        "\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense \n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imutils import paths\n",
        "from random import shuffle, choice\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib as plt\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "import random\n",
        "import glob\n",
        "import time\n",
        "import pandas as pd\n",
        "import math\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib \n",
        "##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ThErfFfNXhU"
      },
      "source": [
        "def load_rgb_data(IMAGE_DIRECTORY,IMAGE_SIZE, shuffle=True):\n",
        "    print(\"Loading images...\")\n",
        "    data = [] #empty list\n",
        "    #categories = []\n",
        "    directories = next(os.walk(IMAGE_DIRECTORY))[1]\n",
        "   \n",
        "    print(directories)\n",
        "    j=0\n",
        "    for diretcory_name in directories:\n",
        "\n",
        "        print(\"Loading {0}\".format(diretcory_name))\n",
        "        file_names = next(os.walk(os.path.join(IMAGE_DIRECTORY, diretcory_name)))[2]\n",
        "        print(\"we will load [\", len(file_names), \"] files from [\",diretcory_name,\"] class ...\" )\n",
        "\n",
        "        for i in range(len(file_names)):\n",
        "\n",
        "          image_name = file_names[i]\n",
        "          image_path = os.path.join(IMAGE_DIRECTORY, diretcory_name, image_name)\n",
        "          if ('.DS_Store' not in image_path):\n",
        "            \n",
        "            category = diretcory_name\n",
        "            label = j\n",
        "            \n",
        "            #get the image\n",
        "            img = Image.open(image_path)\n",
        "            #Images are in RGB format (PIL library)\n",
        "            rgbimg = Image.new(\"RGB\", img.size)\n",
        "            rgbimg.paste(img) \n",
        "            img=rgbimg\n",
        "            #resize the image to be the desired input size of the NN\n",
        "            img = img.resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)\n",
        "            #convert images into np array (input of a neural network)\n",
        "            data.append([np.array(img), label])\n",
        "            #data.append([np.array(img), category])\n",
        "        j=j+1\n",
        "\n",
        "    if (shuffle):\n",
        "      random.shuffle(data)\n",
        "    images = np.array([i[0] for i in data]).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "    labels = np.array([i[1] for i in data])\n",
        "    \n",
        "    print(\"File loading completed.\")\n",
        "\n",
        "    return images, labels, data\n",
        "\n",
        "X_Data, Y_Data , Data = load_rgb_data(dataset, 224 , shuffle=True)\n",
        " \n",
        "#X_Data = X_Data /255.0\n",
        "\n",
        "print(X_Data.shape)\n",
        "print(Y_Data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHDBCogTNctM"
      },
      "source": [
        "**Train, Test, Validation sets..**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2ZWILBrNcLg"
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKVVllQjNkZM"
      },
      "source": [
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw97IC9VNmcs"
      },
      "source": [
        "# set some paths\n",
        "model_dir = Path('/content/drive/MyDrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qce3F6TCNoVD"
      },
      "source": [
        "'''\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P1C0DOSNqOm"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_Data, Y_Data, test_size=0.1,  stratify=Y_Data )\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train) # 0.25 x 0.8 = 0.2\n",
        "X_Data =[]\n",
        "Y_Data =[]\n",
        "#one hit encoding\n",
        "#y_train = np_utils.to_categorical(y_train)\n",
        "#y_test = np_utils.to_categorical(y_test)\n",
        "#y_val = np_utils.to_categorical(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HCT"
      ],
      "metadata": {
        "id": "VZ85TYAWw8Hr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCuiw7bCNsIN"
      },
      "source": [
        "positional_emb = True\n",
        "conv_layers = 2\n",
        "projection_dim = 512\n",
        "\n",
        "num_heads = 1\n",
        "transformer_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 1\n",
        "stochastic_depth_rate = 0.001\n",
        "\n",
        "learning_rate =0.000001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 32\n",
        "num_epochs = 200\n",
        "image_size =  224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6VxN18CNyzu"
      },
      "source": [
        "class CCTTokenizer(layers.Layer):\n",
        "  \n",
        "  def __init__(\n",
        "        self,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        pooling_kernel_size=3,\n",
        "        pooling_stride=2,\n",
        "        num_conv_layers=conv_layers,\n",
        "        num_output_channels=[64, 128],\n",
        "        positional_emb=positional_emb,\n",
        "        **kwargs,\n",
        "    ):\n",
        "       super(CCTTokenizer, self).__init__(**kwargs)\n",
        "       self.positional_emb = positional_emb\n",
        "       \n",
        "  def call(self, images):\n",
        "      outputs = backbone(images)\n",
        "        # After passing the images through our mini-network the spatial dimensions\n",
        "        # are flattened to form sequences.\n",
        "      reshaped = tf.reshape(outputs,\n",
        "                            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),)\n",
        "      return reshaped\n",
        "\n",
        "  def positional_embedding(self, image_size):\n",
        "        # Positional embeddings are optional in CCT. Here, we calculate \n",
        "        # the number of sequences and initialize an `Embedding` layer to\n",
        "        # compute the positional embeddings later.\n",
        "    if self.positional_emb:\n",
        "      dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
        "      dummy_outputs = self.call(dummy_inputs)\n",
        "      sequence_length = tf.shape(dummy_outputs)[1]\n",
        "      projection_dim = tf.shape(dummy_outputs)[-1]\n",
        "      embed_layer = layers.Embedding(\n",
        "          input_dim=sequence_length, output_dim=projection_dim\n",
        "          )\n",
        "      return embed_layer, sequence_length\n",
        "    else:\n",
        "      return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu505IV4N1d5"
      },
      "source": [
        "# Referred from: github.com:rwightman/pytorch-image-models.\n",
        "class StochasticDepth(layers.Layer):\n",
        "    def __init__(self, drop_prop, **kwargs):\n",
        "        super(StochasticDepth, self).__init__(**kwargs)\n",
        "        self.drop_prob = drop_prop\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        if training:\n",
        "            keep_prob = 1 - self.drop_prob\n",
        "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
        "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
        "            random_tensor = tf.floor(random_tensor)\n",
        "            return (x / keep_prob) * random_tensor\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSlU8-5kN3lC"
      },
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.relu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ts69m32N5Zd"
      },
      "source": [
        "# Note the rescaling layer. These layers have pre-defined inference behavior.\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Rescaling(scale=1.0 / 255),\n",
        "        layers.RandomCrop(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMWQjrE9N7Tb"
      },
      "source": [
        "backbone = tf.keras.applications.VGG19(weights=\"imagenet\", input_shape=(224,224,3), include_top=False)\n",
        "#backbone = tf.keras.applications.ResNet50(weights=\"imagenet\", input_shape=(224,224,3), include_top=False)\n",
        "#backbone = tf.keras.applications.EfficientNetB0(weights=\"imagenet\", input_shape=(224,224,3), include_top=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rci85Y14N-2U"
      },
      "source": [
        "for layer in backbone.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmqYvKOvN_Ym"
      },
      "source": [
        "num_classes = 3\n",
        "input_shape = (224, 224, 3)\n",
        "def create_cct_model(\n",
        "    image_size=image_size,\n",
        "    input_shape=input_shape,\n",
        "    num_heads=num_heads,\n",
        "    projection_dim=projection_dim,\n",
        "    transformer_units=transformer_units,\n",
        "    ):\n",
        "\n",
        "    inputs = layers.Input(input_shape)\n",
        "\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "\n",
        "    # Encode patches.\n",
        "    cct_tokenizer = CCTTokenizer()\n",
        "    encoded_patches = cct_tokenizer(inputs)\n",
        "\n",
        "    # Apply positional embedding.\n",
        "    if positional_emb:\n",
        "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
        "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
        "        position_embeddings = pos_embed(positions)\n",
        "        encoded_patches += position_embeddings\n",
        "\n",
        "\n",
        "    # Calculate Stochastic Depth probabilities.\n",
        "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for i in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip connection 1.\n",
        "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
        "\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "\n",
        "        # Skip connection 2.\n",
        "        x3 = StochasticDepth(dpr[i])(x3)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Apply sequence pooling.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
        "    weighted_representation = tf.matmul(\n",
        "        attention_weights, representation, transpose_a=True\n",
        "    )\n",
        "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
        "\n",
        "    # Classify outputs.\n",
        "    logits = tf.nn.softmax(layers.Dense(3)(weighted_representation))\n",
        "    \n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    \n",
        "    #KFold\n",
        "    weight_decay=learning_rate/num_epochs\n",
        "    optimizer = tfa.optimizers.AdamW( learning_rate=learning_rate, weight_decay=weight_decay)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True),\n",
        "            metrics=[\n",
        "                     keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "                     keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),],)\n",
        "  \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcLYfZYaOCPF"
      },
      "source": [
        "\n",
        "#KFold\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import f1_score, cohen_kappa_score, roc_auc_score\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "acc = [] \n",
        "sensitivity = []\n",
        "specificity = []\n",
        "precision = []\n",
        "F1 = []\n",
        "TPR = []\n",
        "FPR = []\n",
        "yhat_probs = []\n",
        "yhat_classes= []\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "\n",
        "for train, val in kfold.split(X_train, y_train):\n",
        "#X_train[train] = X_train, X_train[val] = X_val, y_train[train] = y_train, y_train[val] = y_val\n",
        "  model = create_cct_model()\n",
        "  total = 0\n",
        "  cm = 0\n",
        "  checkpoint_callback = []\n",
        "  checkpoint_filepath = \"\"\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  def exp_decay(num_epochs):\n",
        "      lrate = learning_rate * np.exp(-weight_decay)\n",
        "      return lrate\n",
        "  lr_rate = keras.callbacks.LearningRateScheduler(exp_decay)\n",
        "  \n",
        "  def checkpointF(fold_no):\n",
        "    return {\n",
        "        1: 'vision_____transformer.h5f',\n",
        "        2: 'vision______transformer.h5f',\n",
        "        3: 'vision_______transformer.h5f',\n",
        "        4: 'vision________transformer.h5f'\n",
        "        }.get(fold_no, 'vision_________transformer.h5f')\n",
        "        \n",
        "\n",
        "  checkpoint_filepath = checkpointF(fold_no)\n",
        "  print(checkpoint_filepath)\n",
        "  checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "      model_dir.joinpath(checkpoint_filepath),\n",
        "      monitor=\"val_accuracy\",\n",
        "      save_best_only=True,\n",
        "      save_weights_only=True,\n",
        "      )\n",
        "\n",
        "#X_train[train] = X_train, X_train[val] = X_val, y_train[train] = y_train, y_train[val] = y_val\n",
        "\n",
        "  history = model.fit(\n",
        "      x=X_train[train],\n",
        "      y=y_train[train],\n",
        "      batch_size=batch_size,\n",
        "      validation_data =(X_train[val], y_train[val]),\n",
        "      validation_steps=len(X_train[val])//batch_size,\n",
        "      epochs=num_epochs,\n",
        "      callbacks = [checkpoint_callback, lr_rate]\n",
        "      )\n",
        "  \n",
        "  model.load_weights(checkpoint_filepath)\n",
        "  _, accuracy, top_5_accuracy = model.evaluate(X_test, y_test)\n",
        "  print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "  print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "  \n",
        "  # Generate generalization metrics\n",
        "  model.load_weights(checkpoint_filepath)\n",
        "  scores = model.evaluate(X_test, y_test)\n",
        "  #scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  yhat_probs.append(model.predict(X_test, verbose=0))\n",
        "# predict crisp classes for test set\n",
        "  yhat_classes.append(np.argmax(yhat_probs[fold_no-1], axis=1))\n",
        "#yhat_probs = yhat_probs[:,0]\n",
        "\n",
        "#Multiclass\n",
        "  cm = confusion_matrix(y_test, yhat_classes[fold_no-1])\n",
        "  total = sum(sum(cm))\n",
        "  acc.append((cm[0,0] + cm[1, 1] + cm[2, 2]) / total)\n",
        "  sensitivity.append(cm[2,2]/(cm[2,0] + cm[2,2] + cm[2,1]))\n",
        "  specificity.append((cm[0,0]+ cm[0,1]+ cm[1,0]+ cm[1,1])/((cm[0,0]+ cm[0,1]+ cm[1,0] + cm[1,1] + cm[1,2]+cm[0,2]))\n",
        "  precision.append(cm[2,2]/(cm[2,2] + cm[1,2]+cm[0,2]))\n",
        "  F1.append(2*((precision[fold_no-1] * sensitivity[fold_no-1])/(precision[fold_no-1] + sensitivity[fold_no-1])))\n",
        "  TPR.append(sensitivity[fold_no-1])\n",
        "  FPR.append(1-specificity[fold_no-1])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zznadOf7OMHd"
      },
      "source": [
        "\n",
        "acc_Avrg = 0\n",
        "sensitivity_Avrg = 0\n",
        "specificity_Avrg = 0\n",
        "precision_Avrg = 0\n",
        "F1_Avrg = 0\n",
        "TPR_Avrg = 0\n",
        "FPR_Avrg = 0\n",
        "for i in range(5):\n",
        "  acc_Avrg += acc[i]\n",
        "  sensitivity_Avrg += sensitivity[i]\n",
        "  specificity_Avrg += specificity[i]\n",
        "  precision_Avrg += precision[i]\n",
        "  F1_Avrg += F1[i]\n",
        "  TPR_Avrg += TPR[i]\n",
        "  FPR_Avrg += FPR[i]\n",
        "\n",
        "acc_Avrg = acc_Avrg/5\n",
        "print(f'acc_Avrg = {acc_Avrg}')\n",
        "print(f'acc = {acc}')\n",
        "\n",
        "sensitivity_Avrg = sensitivity_Avrg/5\n",
        "print(f'sensitivity_Avrg = {sensitivity_Avrg}')\n",
        "print(f'sensitivity = {sensitivity}')\n",
        "\n",
        "specificity_Avrg = specificity_Avrg/5\n",
        "print(f'specificity_Avrg = {specificity_Avrg}')\n",
        "print(f'specificity = {specificity}')\n",
        "\n",
        "precision_Avrg = precision_Avrg/5\n",
        "print(f'precision_Avrg = {precision_Avrg}')\n",
        "print(f'precision = {precision}')\n",
        "\n",
        "F1_Avrg = F1_Avrg/5\n",
        "print(f'F1_Avrg = {F1_Avrg}')\n",
        "print(f'F1 = {F1}')\n",
        "\n",
        "TPR_Avrg = TPR_Avrg/5\n",
        "print(f'TPR_Avrg = {TPR_Avrg}')\n",
        "print(f'TPR = {TPR}')\n",
        "\n",
        "FPR_Avrg = FPR_Avrg/5\n",
        "print(f'FPR_Avrg = {FPR_Avrg}')\n",
        "print(f'FPR = {FPR}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM7-LwjqOMqN"
      },
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9JO6BPNOP2o"
      },
      "source": [
        "plt.plot(history.history[\"accuracy\"], label=\"train_accuracy\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train and Validation Accuracy Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import f1_score, cohen_kappa_score, roc_auc_score\n",
        "yhat_probs = cct_model.predict(X_test, verbose=0)\n",
        "# predict crisp classes for test set\n",
        "yhat_classes = np.argmax(yhat_probs, axis=1)\n",
        "#yhat_probs = yhat_probs[:,0]"
      ],
      "metadata": {
        "id": "MygMjMRb2xO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# roc curve for classes\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 3\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, yhat_probs[:,i], pos_label=i)\n",
        "    #roc_auc_score(y_test, yhat_probs, multi_class='ovo', average='weighted')\n",
        "    #print(i)\n",
        "    \n",
        "\n",
        "# plotting    \n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class CAP vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class NORMAL vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class CP vs Rest')\n",
        "plt.title('Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);"
      ],
      "metadata": {
        "id": "MVRvGcr520ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Multiclass CM\n",
        "cm = confusion_matrix(y_test, yhat_classes)\n",
        "total = sum(sum(cm))\n",
        "acc = (cm[0,0] + cm[1, 1] + cm[2, 2]) / total\n",
        "\n",
        "sensitivity = cm[2,2]/(cm[2,0] + cm[2,2] + cm[2,1])\n",
        "specificity = (cm[0,0]+ cm[0,1]+ cm[1,0]+ cm[1,1])/((cm[0,0]+ cm[0,1]+ cm[1,0] + cm[1,1] + cm[1,2]+cm[0,2])\n",
        "precision = cm[2,2]/(cm[2,2] + cm[1,2]+cm[0,2])\n",
        "F1 = 2*((precision * sensitivity)/(precision + sensitivity))\n",
        "print(cm)\n",
        "print(\"acc: {:.4f}\".format(acc))\n",
        "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
        "print(\"specificity: {:.4f}\".format(specificity))\n",
        "print(\"precision: {:.4f}\".format(precision))\n",
        "print(\"f1: {:.4f}\".format(F1))\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index = ['CAP', 'NORMAL', 'CP'], \n",
        "                     columns = ['CAP', 'NORMAL', 'CP'])"
      ],
      "metadata": {
        "id": "9oMFuaFp2-Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the confusion matrix\n",
        "import seaborn as sns;\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_df, annot=True, cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actal Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YOZ4zf2MkMx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AUC value\n",
        "from sklearn.metrics import roc_auc_score\n",
        "r = roc_auc_score(y_test, yhat_probs, average='macro', sample_weight=None, max_fpr=None, multi_class='ovr', labels=None)\n",
        "#fpr[i], tpr[i], thresh[i] = roc_curve(y_test, yhat_probs[:,i], pos_label=i)\n",
        "print(r)"
      ],
      "metadata": {
        "id": "-W7BuC9T2nrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning"
      ],
      "metadata": {
        "id": "EUs6c0kwrCoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "INIT_LR = .0001\n",
        "epochs = 100\n",
        "BatchSize = 64\n",
        "backbone = tf.keras.applications.VGG19(weights=\"imagenet\", input_shape=(224,224,3), include_top=False)\n",
        "#backbone = tf.keras.applications.ResNet50(weights=\"imagenet\", input_shape=(224,224,3), include_top=False)\n",
        "#backbone = tf.keras.applications.EfficientNetB0(weights=\"imagenet\", input_shape=(224,224,3), include_top=False)"
      ],
      "metadata": {
        "id": "UiNEV4HorT3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  headModel = baseModel.output\n",
        "  headModel = AveragePooling2D(pool_size=(4,4))(headModel)\n",
        "  headModel = Flatten(name=\"flatten\")(headModel)\n",
        "  headModel = Dense(64, activation=\"relu\")(headModel)\n",
        "  headModel = Dropout(0.5)(headModel)\n",
        "  headModel = Dense(3, activation=\"softmax\")(headModel)\n",
        "  model = Model(inputs = baseModel.input, outputs=headModel)\n",
        "\n",
        "  for layer in baseModel.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  weight_decay=INIT_LR/ epochs\n",
        "  optimizer = tfa.optimizers.AdamW( learning_rate=INIT_LR, weight_decay=weight_decay)\n",
        "    \n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=keras.losses.SparseCategoricalCrossentropy(\n",
        "          from_logits=False),\n",
        "          metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "                   keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\")])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "xOVkDg7duwsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNPR6wLuGxrK"
      },
      "source": [
        "#KFold\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import f1_score, cohen_kappa_score, roc_auc_score\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "acc = [] \n",
        "sensitivity = []\n",
        "specificity = []\n",
        "precision = []\n",
        "F1 = []\n",
        "TPR = []\n",
        "FPR = []\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "weight_decay=INIT_LR/ epochs\n",
        "for train, val in kfold.split(X_train, y_train):\n",
        "#X_train[train] = X_train, X_train[val] = X_val, y_train[train] = y_train, y_train[val] = y_val\n",
        "  model = create_model()\n",
        "  total = 0\n",
        "  cm = 0\n",
        "  checkpoint_callback = []\n",
        "  checkpoint_filepath = \"\"\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "  # Fit data to model\n",
        "  def exp_decay(num_epochs):\n",
        "    lrate = INIT_LR * np.exp(-weight_decay)\n",
        "    return lrate\n",
        "\n",
        "  lr_rate = keras.callbacks.LearningRateScheduler(exp_decay)\n",
        "    \n",
        "  \n",
        "  def checkpointF(fold_no):\n",
        "    return {\n",
        "        1: 'covidCXRcnnN.h5f',\n",
        "        2: 'covidCXRcnnNN.h5f',\n",
        "        3: 'covidCXRcnnNNN.h5f',\n",
        "        4: 'covidCXRcnnNNNN.h5f'\n",
        "        }.get(fold_no, 'covidCXRcnnNNNNN.h5f')\n",
        "        \n",
        "\n",
        "  checkpoint_filepath = checkpointF(fold_no)\n",
        "  print(checkpoint_filepath)\n",
        "  checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "      model_dir.joinpath(checkpoint_filepath),\n",
        "      monitor=\"val_accuracy\",\n",
        "      save_best_only=True,\n",
        "      save_weights_only=True,\n",
        "      )\n",
        "\n",
        "#X_train[train] = X_train, X_train[val] = X_val, y_train[train] = y_train, y_train[val] = y_val\n",
        "\n",
        "  history = model.fit(\n",
        "      x=X_train[train],\n",
        "      y=y_train[train],\n",
        "      batch_size=BatchSize,\n",
        "      validation_data =(X_train[val], y_train[val]),\n",
        "      validation_steps=len(X_train[val])//BatchSize,\n",
        "      epochs=epochs,\n",
        "      callbacks = [checkpoint_callback, lr_rate]\n",
        "      )\n",
        "  \n",
        "\n",
        "  '''\n",
        "  model.load_weights(checkpoint_filepath)\n",
        "  _, accuracy, top_5_accuracy = model.evaluate(X_test, y_test)\n",
        "  print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "  print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "  '''\n",
        "  # Generate generalization metrics\n",
        "  model.load_weights(checkpoint_filepath)\n",
        "  scores = model.evaluate(X_test, y_test)\n",
        "  #scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  yhat_probs = model.predict(X_test, verbose=0)\n",
        "# predict crisp classes for test set\n",
        "  yhat_classes = np.argmax(yhat_probs, axis=1)\n",
        "#yhat_probs = yhat_probs[:,0]\n",
        "\n",
        "#Multiclass\n",
        "  cm = confusion_matrix(y_test, yhat_classes)\n",
        "  total = sum(sum(cm))\n",
        "  acc.append((cm[0,0] + cm[1, 1] + cm[2, 2]) / total)\n",
        "  sensitivity.append(cm[2,2]/(cm[2,0] + cm[2,2] + cm[2,1]))\n",
        "  specificity.append((cm[0,0]+ cm[0,1]+ cm[1,0]+ cm[1,1])/((cm[0,0]+ cm[0,1]+ cm[1,0] + cm[1,1] + cm[1,2]+cm[0,2]))\n",
        "  precision.append(cm[2,2]/(cm[2,2] + cm[1,2]+cm[0,2]))\n",
        "  F1.append(2*((precision[fold_no-1] * sensitivity[fold_no-1])/(precision[fold_no-1] + sensitivity[fold_no-1])))\n",
        "  TPR.append(sensitivity[fold_no-1])\n",
        "  FPR.append(1-specificity[fold_no-1])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_Avrg = 0\n",
        "sensitivity_Avrg = 0\n",
        "specificity_Avrg = 0\n",
        "precision_Avrg = 0\n",
        "F1_Avrg = 0\n",
        "TPR_Avrg = 0\n",
        "FPR_Avrg = 0\n",
        "for i in range(5):\n",
        "  acc_Avrg += acc[i]\n",
        "  sensitivity_Avrg += sensitivity[i]\n",
        "  specificity_Avrg += specificity[i]\n",
        "  precision_Avrg += precision[i]\n",
        "  F1_Avrg += F1[i]\n",
        "  TPR_Avrg += TPR[i]\n",
        "  FPR_Avrg += FPR[i]\n",
        "\n",
        "acc_Avrg = acc_Avrg/5\n",
        "print(f'acc_Avrg = {acc_Avrg}')\n",
        "print(f'acc = {acc}')\n",
        "\n",
        "sensitivity_Avrg = sensitivity_Avrg/5\n",
        "print(f'sensitivity_Avrg = {sensitivity_Avrg}')\n",
        "print(f'sensitivity = {sensitivity}')\n",
        "\n",
        "specificity_Avrg = specificity_Avrg/5\n",
        "print(f'specificity_Avrg = {specificity_Avrg}')\n",
        "print(f'specificity = {specificity}')\n",
        "\n",
        "precision_Avrg = precision_Avrg/5\n",
        "print(f'precision_Avrg = {precision_Avrg}')\n",
        "print(f'precision = {precision}')\n",
        "\n",
        "F1_Avrg = F1_Avrg/5\n",
        "print(f'F1_Avrg = {F1_Avrg}')\n",
        "print(f'F1 = {F1}')\n",
        "\n",
        "TPR_Avrg = TPR_Avrg/5\n",
        "print(f'TPR_Avrg = {TPR_Avrg}')\n",
        "print(f'TPR = {TPR}')\n",
        "\n",
        "FPR_Avrg = FPR_Avrg/5\n",
        "print(f'FPR_Avrg = {FPR_Avrg}')\n",
        "print(f'FPR = {FPR}')"
      ],
      "metadata": {
        "id": "KfO4sTHXuz0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6conC72xu_Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"accuracy\"], label=\"train_accuracy\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train and Validation Accuracy Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yd6ZL3jEvDKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import f1_score, cohen_kappa_score, roc_auc_score\n",
        "yhat_probs = cct_model.predict(X_test, verbose=0)\n",
        "# predict crisp classes for test set\n",
        "yhat_classes = np.argmax(yhat_probs, axis=1)\n",
        "#yhat_probs = yhat_probs[:,0]"
      ],
      "metadata": {
        "id": "VmdXAwtKvGZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# roc curve for classes\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 3\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, yhat_probs[:,i], pos_label=i)\n",
        "    #roc_auc_score(y_test, yhat_probs, multi_class='ovo', average='weighted')\n",
        "    #print(i)\n",
        "    \n",
        "\n",
        "# plotting    \n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class CAP vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class NORMAL vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class CP vs Rest')\n",
        "plt.title('Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);"
      ],
      "metadata": {
        "id": "PvO1g_-HvG6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Multiclass CM\n",
        "cm = confusion_matrix(y_test, yhat_classes)\n",
        "total = sum(sum(cm))\n",
        "acc = (cm[0,0] + cm[1, 1] + cm[2, 2]) / total\n",
        "\n",
        "sensitivity = cm[2,2]/(cm[2,0] + cm[2,2] + cm[2,1])\n",
        "specificity = (cm[0,0]+ cm[0,1]+ cm[1,0]+ cm[1,1])/((cm[0,0]+ cm[0,1]+ cm[1,0] + cm[1,1] + cm[1,2]+cm[0,2])\n",
        "precision = cm[2,2]/(cm[2,2] + cm[1,2]+cm[0,2])\n",
        "F1 = 2*((precision * sensitivity)/(precision + sensitivity))\n",
        "print(cm)\n",
        "print(\"acc: {:.4f}\".format(acc))\n",
        "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
        "print(\"specificity: {:.4f}\".format(specificity))\n",
        "print(\"precision: {:.4f}\".format(precision))\n",
        "print(\"f1: {:.4f}\".format(F1))\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index = ['CAP', 'NORMAL', 'CP'], \n",
        "                     columns = ['CAP', 'NORMAL', 'CP'])"
      ],
      "metadata": {
        "id": "MWITfMGQvPHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the confusion matrix\n",
        "import seaborn as sns;\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_df, annot=True, cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actal Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E8Kg4CfzvTDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AUC value\n",
        "from sklearn.metrics import roc_auc_score\n",
        "r = roc_auc_score(y_test, yhat_probs, average='macro', sample_weight=None, max_fpr=None, multi_class='ovr', labels=None)\n",
        "#fpr[i], tpr[i], thresh[i] = roc_curve(y_test, yhat_probs[:,i], pos_label=i)\n",
        "print(r)"
      ],
      "metadata": {
        "id": "PKeOz6r8vWOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CCT"
      ],
      "metadata": {
        "id": "8TK0MtuwrOAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positional_emb = True\n",
        "conv_layers = 2\n",
        "projection_dim = 128\n",
        "\n",
        "num_heads = 2\n",
        "transformer_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 2\n",
        "stochastic_depth_rate = 0.1\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 100\n",
        "image_size = 32"
      ],
      "metadata": {
        "id": "ZjQaf3RGrVq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "input_shape = (32, 32, 3)\n",
        "class CCTTokenizer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        pooling_kernel_size=3,\n",
        "        pooling_stride=2,\n",
        "        num_conv_layers=conv_layers,\n",
        "        num_output_channels=[64, 128],\n",
        "        positional_emb=positional_emb,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(CCTTokenizer, self).__init__(**kwargs)\n",
        "\n",
        "        # This is our tokenizer.\n",
        "        self.conv_model = keras.Sequential()\n",
        "        for i in range(num_conv_layers):\n",
        "            self.conv_model.add(\n",
        "                layers.Conv2D(\n",
        "                    num_output_channels[i],\n",
        "                    kernel_size,\n",
        "                    stride,\n",
        "                    padding=\"valid\",\n",
        "                    use_bias=False,\n",
        "                    activation=\"relu\",\n",
        "                    kernel_initializer=\"he_normal\",\n",
        "                )\n",
        "            )\n",
        "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
        "            self.conv_model.add(\n",
        "                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
        "            )\n",
        "\n",
        "        self.positional_emb = positional_emb\n",
        "\n",
        "    def call(self, images):\n",
        "        outputs = self.conv_model(images)\n",
        "        # After passing the images through our mini-network the spatial dimensions\n",
        "        # are flattened to form sequences.\n",
        "        reshaped = tf.reshape(\n",
        "            outputs,\n",
        "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
        "        )\n",
        "        return reshaped\n",
        "\n",
        "    def positional_embedding(self, image_size):\n",
        "        # Positional embeddings are optional in CCT. Here, we calculate\n",
        "        # the number of sequences and initialize an `Embedding` layer to\n",
        "        # compute the positional embeddings later.\n",
        "        if self.positional_emb:\n",
        "            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
        "            dummy_outputs = self.call(dummy_inputs)\n",
        "            sequence_length = tf.shape(dummy_outputs)[1]\n",
        "            projection_dim = tf.shape(dummy_outputs)[-1]\n",
        "\n",
        "            embed_layer = layers.Embedding(\n",
        "                input_dim=sequence_length, output_dim=projection_dim\n",
        "            )\n",
        "            return embed_layer, sequence_length\n",
        "        else:\n",
        "            return None"
      ],
      "metadata": {
        "id": "brOFGk6PrYwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Referred from: github.com:rwightman/pytorch-image-models.\n",
        "class StochasticDepth(layers.Layer):\n",
        "    def __init__(self, drop_prop, **kwargs):\n",
        "        super(StochasticDepth, self).__init__(**kwargs)\n",
        "        self.drop_prob = drop_prop\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        if training:\n",
        "            keep_prob = 1 - self.drop_prob\n",
        "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
        "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
        "            random_tensor = tf.floor(random_tensor)\n",
        "            return (x / keep_prob) * random_tensor\n",
        "        return x"
      ],
      "metadata": {
        "id": "mfZFPmhGrb1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "6I1LhFoxrdwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the rescaling layer. These layers have pre-defined inference behavior.\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Rescaling(scale=1.0 / 255),\n",
        "        layers.RandomCrop(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")"
      ],
      "metadata": {
        "id": "k9QHEKB5rfcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "input_shape = (32, 32, 3)\n",
        "def create_cct_model(\n",
        "    image_size=image_size,\n",
        "    input_shape=input_shape,\n",
        "    num_heads=num_heads,\n",
        "    projection_dim=projection_dim,\n",
        "    transformer_units=transformer_units,\n",
        "    ):\n",
        "\n",
        "    inputs = layers.Input(input_shape)\n",
        "\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "\n",
        "    # Encode patches.\n",
        "    cct_tokenizer = CCTTokenizer()\n",
        "    encoded_patches = cct_tokenizer(inputs)\n",
        "\n",
        "    # Apply positional embedding.\n",
        "    if positional_emb:\n",
        "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
        "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
        "        position_embeddings = pos_embed(positions)\n",
        "        encoded_patches += position_embeddings\n",
        "\n",
        "\n",
        "    # Calculate Stochastic Depth probabilities.\n",
        "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for i in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip connection 1.\n",
        "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
        "\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "\n",
        "        # Skip connection 2.\n",
        "        x3 = StochasticDepth(dpr[i])(x3)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Apply sequence pooling.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
        "    weighted_representation = tf.matmul(\n",
        "        attention_weights, representation, transpose_a=True\n",
        "    )\n",
        "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
        "\n",
        "    # Classify outputs.\n",
        "    logits = tf.nn.softmax(layers.Dense(3)(weighted_representation))\n",
        "    \n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    \n",
        "    #222\n",
        "    weight_decay=learning_rate/num_epochs\n",
        "    optimizer = tfa.optimizers.AdamW( learning_rate=learning_rate, weight_decay=weight_decay)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True),\n",
        "            metrics=[\n",
        "                     keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "                     keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),],)\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "BqzWjS5orhhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import f1_score, cohen_kappa_score, roc_auc_score\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "acc = [] \n",
        "sensitivity = []\n",
        "specificity = []\n",
        "precision = []\n",
        "F1 = []\n",
        "TPR = []\n",
        "FPR = []\n",
        "yhat_probs = []\n",
        "yhat_classes= []\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "\n",
        "for train, val in kfold.split(X_train, y_train):\n",
        "#X_train[train] = X_train, X_train[val] = X_val, y_train[train] = y_train, y_train[val] = y_val\n",
        "  model = create_cct_model()\n",
        "  total = 0\n",
        "  cm = 0\n",
        "  checkpoint_callback = []\n",
        "  checkpoint_filepath = \"\"\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  def exp_decay(num_epochs):\n",
        "      lrate = learning_rate * np.exp(-weight_decay)\n",
        "      return lrate\n",
        "  lr_rate = keras.callbacks.LearningRateScheduler(exp_decay)\n",
        "  \n",
        "  def checkpointF(fold_no):\n",
        "    return {\n",
        "        1: 'vision_____transformer.h5f',\n",
        "        2: 'vision______transformer.h5f',\n",
        "        3: 'vision_______transformer.h5f',\n",
        "        4: 'vision________transformer.h5f'\n",
        "        }.get(fold_no, 'vision_________transformer.h5f')\n",
        "        \n",
        "\n",
        "  checkpoint_filepath = checkpointF(fold_no)\n",
        "  print(checkpoint_filepath)\n",
        "  checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "      model_dir.joinpath(checkpoint_filepath),\n",
        "      monitor=\"val_accuracy\",\n",
        "      save_best_only=True,\n",
        "      save_weights_only=True,\n",
        "      )\n",
        "\n",
        "#X_train[train] = X_train, X_train[val] = X_val, y_train[train] = y_train, y_train[val] = y_val\n",
        "\n",
        "  history = model.fit(\n",
        "      x=X_train[train],\n",
        "      y=y_train[train],\n",
        "      batch_size=batch_size,\n",
        "      validation_data =(X_train[val], y_train[val]),\n",
        "      validation_steps=len(X_train[val])//batch_size,\n",
        "      epochs=num_epochs,\n",
        "      callbacks = [checkpoint_callback, lr_rate]\n",
        "      )\n",
        "  \n",
        "  # Generate generalization metrics\n",
        "  model.load_weights(checkpoint_filepath)\n",
        "  scores = model.evaluate(X_test, y_test)\n",
        "  #scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  yhat_probs.append(model.predict(X_test, verbose=0))\n",
        "# predict crisp classes for test set\n",
        "  yhat_classes.append(np.argmax(yhat_probs[fold_no-1], axis=1))\n",
        "#yhat_probs = yhat_probs[:,0]\n",
        "\n",
        "#Multiclass\n",
        "  cm = confusion_matrix(y_test, yhat_classes[fold_no-1])\n",
        "  total = sum(sum(cm))\n",
        "  acc.append((cm[0,0] + cm[1, 1] + cm[2, 2]) / total)\n",
        "  sensitivity.append(cm[2,2]/(cm[2,0] + cm[2,2] + cm[2,1]))\n",
        "  specificity.append((cm[0,0]+ cm[0,1]+ cm[1,0]+ cm[1,1])/((cm[0,0]+ cm[0,1]+ cm[1,0] + cm[1,1] + cm[1,2]+cm[0,2]))\n",
        "  precision.append(cm[2,2]/(cm[2,2] + cm[1,2]+cm[0,2]))\n",
        "  F1.append(2*((precision[fold_no-1] * sensitivity[fold_no-1])/(precision[fold_no-1] + sensitivity[fold_no-1])))\n",
        "  TPR.append(sensitivity[fold_no-1])\n",
        "  FPR.append(1-specificity[fold_no-1])\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "  # == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "id": "oRA0vD0erlFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_Avrg = 0\n",
        "sensitivity_Avrg = 0\n",
        "specificity_Avrg = 0\n",
        "precision_Avrg = 0\n",
        "F1_Avrg = 0\n",
        "TPR_Avrg = 0\n",
        "FPR_Avrg = 0\n",
        "for i in range(5):\n",
        "  acc_Avrg += acc[i]\n",
        "  sensitivity_Avrg += sensitivity[i]\n",
        "  specificity_Avrg += specificity[i]\n",
        "  precision_Avrg += precision[i]\n",
        "  F1_Avrg += F1[i]\n",
        "  TPR_Avrg += TPR[i]\n",
        "  FPR_Avrg += FPR[i]\n",
        "\n",
        "acc_Avrg = acc_Avrg/5\n",
        "print(f'acc_Avrg = {acc_Avrg}')\n",
        "print(f'acc = {acc}')\n",
        "\n",
        "sensitivity_Avrg = sensitivity_Avrg/5\n",
        "print(f'sensitivity_Avrg = {sensitivity_Avrg}')\n",
        "print(f'sensitivity = {sensitivity}')\n",
        "\n",
        "specificity_Avrg = specificity_Avrg/5\n",
        "print(f'specificity_Avrg = {specificity_Avrg}')\n",
        "print(f'specificity = {specificity}')\n",
        "\n",
        "precision_Avrg = precision_Avrg/5\n",
        "print(f'precision_Avrg = {precision_Avrg}')\n",
        "print(f'precision = {precision}')\n",
        "\n",
        "F1_Avrg = F1_Avrg/5\n",
        "print(f'F1_Avrg = {F1_Avrg}')\n",
        "print(f'F1 = {F1}')\n",
        "\n",
        "TPR_Avrg = TPR_Avrg/5\n",
        "print(f'TPR_Avrg = {TPR_Avrg}')\n",
        "print(f'TPR = {TPR}')\n",
        "\n",
        "FPR_Avrg = FPR_Avrg/5\n",
        "print(f'FPR_Avrg = {FPR_Avrg}')\n",
        "print(f'FPR = {FPR}')\n"
      ],
      "metadata": {
        "id": "sXvKM9PMroTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hOoG1sdbrrcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"accuracy\"], label=\"train_accuracy\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train and Validation Accuracy Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PA5DeLWRru12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import f1_score, cohen_kappa_score, roc_auc_score\n",
        "yhat_probs = model.predict(X_test, verbose=0)\n",
        "# predict crisp classes for test set\n",
        "yhat_classes = np.argmax(yhat_probs, axis=1)"
      ],
      "metadata": {
        "id": "9wZswVULrvgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# roc curve for classes\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 3\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, yhat_probs[:,i], pos_label=i)\n",
        "    #roc_auc_score(y_test, yhat_probs, multi_class='ovo', average='weighted')\n",
        "    #print(i)\n",
        "    \n",
        "\n",
        "# plotting    \n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class CAP vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class NORMAL vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class CP vs Rest')\n",
        "plt.title('Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);   "
      ],
      "metadata": {
        "id": "XxyUBj_Mr0ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Multiclass CM\n",
        "cm = confusion_matrix(y_test, yhat_classes)\n",
        "total = sum(sum(cm))\n",
        "acc = (cm[0,0] + cm[1, 1] + cm[2, 2]) / total\n",
        "\n",
        "sensitivity = cm[2,2]/(cm[2,0] + cm[2,2] + cm[2,1])\n",
        "specificity = (cm[0,0]+ cm[0,1]+ cm[1,0]+ cm[1,1])/((cm[0,0]+ cm[0,1]+ cm[1,0] + cm[1,1] + cm[1,2]+cm[0,2])\n",
        "precision = cm[2,2]/(cm[2,2] + cm[1,2]+cm[0,2])\n",
        "F1 = 2*((precision * sensitivity)/(precision + sensitivity))\n",
        "print(cm)\n",
        "print(\"acc: {:.4f}\".format(acc))\n",
        "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
        "print(\"specificity: {:.4f}\".format(specificity))\n",
        "print(\"precision: {:.4f}\".format(precision))\n",
        "print(\"f1: {:.4f}\".format(F1))\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index = ['CAP', 'NORMAL', 'CP'], \n",
        "                     columns = ['CAP', 'NORMAL', 'CP'])"
      ],
      "metadata": {
        "id": "gYHONckJuDg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the confusion matrix\n",
        "import seaborn as sns;\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_df, annot=True, cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actal Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YGDS4GRKuEBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AUC value\n",
        "from sklearn.metrics import roc_auc_score\n",
        "r = roc_auc_score(y_test, yhat_probs, average='macro', sample_weight=None, max_fpr=None, multi_class='ovr', labels=None)\n",
        "#fpr[i], tpr[i], thresh[i] = roc_curve(y_test, yhat_probs[:,i], pos_label=i)\n",
        "print(r)"
      ],
      "metadata": {
        "id": "A6vSom5kuG71"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}